{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c1691e",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to get a practical understanding of how neural networks work in code, not relying on PyTorch/TensorFlow.\n",
    "Reason why I chose to do this small project is due to the fact that I understand NN, Convolutional layers, Attention heads, though I always rely on libraries to implement them. \n",
    "Hopefully, this will enable me to slowly grasp deeper understandings of neural networks to eventually allow me to create custom tasks by myself.\n",
    "\n",
    "Expectations:\n",
    "- Learn granular details about Neural Networks and document them on this notebook\n",
    "- Primarily use numpy for most of the code\n",
    "- No PyTorch or TF\n",
    "- I will use simple datasets such as MNIST number dataset to evaluate the model\n",
    "\n",
    "Future Work:\n",
    "- Create a simple CNN from scratch (for the same reason mentioned above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a05187d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba2354",
   "metadata": {},
   "source": [
    "### Starting with a simple neuron\n",
    "\n",
    "\n",
    "Thinking about how it looks it should be very similar to linear regression.\n",
    "\n",
    "- we have **y_hat** which is the predicted value\n",
    "- we have all the **features**: x1, x2, x3, x4, x5... xn \n",
    "- each **feature** is multiplied by its corresponding **weight**: w1, w2, ... wn.\n",
    "- Lastly, we also have the **bias**, which helps the linear regression task be more flexible in terms of finding the best fitting line.\n",
    "\n",
    "##### in the example of having 5 features, we will have something similar to this equation\n",
    "y_hat = x1w1 + x2w2 + x3w3 + x4w4 + x5w5 + b\n",
    "\n",
    "In practicality, to make our lives easier, what we do in neural networks is using separated matrices to represent both features, weights and biases.\n",
    "\n",
    "This results in something that looks like: \n",
    "\n",
    "y_hat = X * W.T + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0a218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4   2.33  2.913 1.92  3.61 ]\n"
     ]
    }
   ],
   "source": [
    "#to code a simple neuron, I will start by choosing arbitrary values to figure out a simple way to write my code.\n",
    "#once again, this is strictly for me to learn how to code a neuron given no examples or libraries outside of numpy or matplotlib\n",
    "\n",
    "#let's create a simple inpt for our neuron\n",
    "simple_input = np.array([1.1, 2.6, 6.3, 3.1, 1.9])\n",
    "\n",
    "#let's create the weights for our neuron\n",
    "simple_weights = np.array([0.5, -0.2, 0.01, -0.3, 0.4])\n",
    "\n",
    "#choosing a random bias value\n",
    "b = 2.85\n",
    "\n",
    "#referecing my notes above, we should transpose the weight vector to be able to multiple between simple_input and simple_weight\n",
    "#intuitively, transposing doesn't matter in 1 dimensional vectors since the shapes naturally align, but 2D and above it should be used, so let's build a good habit\n",
    "y_hat = simple_input * simple_weights.T + b\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d11ca9",
   "metadata": {},
   "source": [
    "The result above is an obvious mistake, we should be getting a scalar value instead.\n",
    "My understanding is that we would do element-wise multiplication, into adding all of the values together. Basically collapse all values into 1 scalar value.\n",
    "\n",
    "Looking at documentation from numpy: https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n",
    "\n",
    "The function I should use is np.dot(), it does exactly what we're looking for\n",
    "\n",
    "Weighted sum: 0.55 + (-0.52) + 0.063 + (-0.93) + 0.76 -> -0.077 + 2.85 = 2.773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.773\n"
     ]
    }
   ],
   "source": [
    "#fixing the code, we get\n",
    "y_hat = np.dot(simple_input, simple_weights.T)+ b\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c3532",
   "metadata": {},
   "source": [
    "In an attempt to create a resemblance of a neural networks, I will try to create a simple example of a layer of 5 neurons feeding their inputs to a singular neuron in the following layer\n",
    "\n",
    "*insert equation eventually*\n",
    "\n",
    "Notes: \n",
    "- As I was reading the `np.dot()` documentation, I saw their examples contain `np.arange()` and `np.reshape()` which can definitely help me with writing this code\n",
    "- Disadvantage of `np.arange()` is that there's no randomness, so did further digging and found `np.random.rand()` and `np.random.randn()`\n",
    "- `np.random.rand()` - giving us random numbers between [0,1).\n",
    "- `np.random.randn()` - giving us random numbers between -1 to 1. \n",
    "- To make our code easily written without much clutter, I am thinking about creating a large 2D matrix hosting each of our neurons in layer 1 as a row\n",
    "\n",
    "Intuitively, I am thinking about using `np.random.rand()` for feature inputs to simulate normalized data, while using `np.random.randn()` for weights since weights can be both positive and negative. I know in practice weights are not always betweeen -1 to 1, however, it makes sense to initialize them as such at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52adb271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our input:\n",
      " [0.20571639 0.75373995 0.52000017 0.61077481 0.92073805]\n",
      "layer1 neurons:\n",
      " [[ 0.07893618 -0.32123298 -0.53707044  0.23327483 -0.52434425]\n",
      " [ 0.81547935 -1.31076576  0.30052341 -0.04702089  2.31393213]\n",
      " [ 0.9023737  -0.30687239 -0.07008626 -0.88086836  0.73075758]\n",
      " [-0.53679917 -1.88899984 -1.50660695 -2.56780042 -1.74279219]\n",
      " [ 0.44676928  0.70542972 -1.51599587 -0.66946737 -0.62196441]]\n",
      "layer1 biases: [0.6482886  0.03822854 0.21075823 0.28566933 0.67445388]\n"
     ]
    }
   ],
   "source": [
    "#create random 'normalized' 5 inputs\n",
    "layer1_inputs = np.random.rand(5)\n",
    "\n",
    "#creating our layer1 2D matrix, each neuron is a row\n",
    "layer1_neurons = np.random.randn(25).reshape(5,5)\n",
    "\n",
    "#biases for the neurons in layer 1\n",
    "layer1_biases = np.random.rand(5)\n",
    "\n",
    "#print our variables\n",
    "print('our input:\\n', layer1_inputs)\n",
    "\n",
    "print('layer1 neurons:\\n', layer1_neurons)\n",
    "\n",
    "print('layer1 biases:', layer1_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05743bca",
   "metadata": {},
   "source": [
    "Now that we set up all that we need, let's emulate a small feedforward logic (without activation yet).\n",
    "\n",
    "1. Given 5 input features, we will use `np.dot()` to multiple between the feature vector to the neuron matrix in `layer 1`.\n",
    "2. We will then create `layer 2 neuron` with `np.random.randn()` - 5 different weights once again (5 input features).\n",
    "3. Now, to emulate the feedforward concept, we will take the result of step 1, and treat it as input feature vector to `layer 2 neuron`.\n",
    "4. Repeat the usage of `np.dot()` on the new input vector and `layer 2 neuron`, and we should get a singular scalar value.\n",
    "\n",
    "NOTE:\n",
    "- To use activation function, we will have to pass the resultant feature vector of layer 1 into some activation function method prior to continuing to step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1532b270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 2 neuron weights: [-0.17380856 -2.47304544 -0.14746777 -2.26227928  1.88420965]\n",
      "the final output of our small feedforward experiment: [7.44955699]\n"
     ]
    }
   ],
   "source": [
    "#compute the weighted sum of input and neuron in layer 2\n",
    "layer1_output = np.dot(layer1_inputs, layer1_neurons.T) + layer1_biases\n",
    "\n",
    "#create the neuron weights and bias for layer 2\n",
    "layer2_neuron1 = np.random.randn(5)\n",
    "layer2_bias = np.random.rand(1)\n",
    "\n",
    "#print neuron 1 in layer 2 for visuals\n",
    "print('layer 2 neuron weights:', layer2_neuron1)\n",
    "\n",
    "#calculate and print the final output\n",
    "final_output = np.dot(layer1_output, layer2_neuron1.T) + layer2_bias\n",
    "\n",
    "print('the final output of our small feedforward experiment:', final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e947146",
   "metadata": {},
   "source": [
    "I had to debug a simple issue when trying to implement many neurons in layer 2 instead of just one (in earlier cell).\n",
    "\n",
    "Here is what I learned. When deciding to create many neurons I can approach it in two angles: \n",
    "1. (num of neurons, weights per neuron)\n",
    "2. (weights per neuron, num of neurons)\n",
    "\n",
    "This choice impacts the decision of my future architecture due to how matrix multiplications work.\n",
    "In my example, if i were to code my layer2 as (5 weights, 8 neurons) and try to dot product with (5 inputs), I'd have to transpose the neurons/weights matrix to make it compatible.\n",
    "\n",
    "Following a standard of (weights, neurons) and `np.dot(input, weights)` alleviates the need to transpose some of the times, decreasing computational costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab211b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurons in layer 2:\n",
      " [[-0.7624961  -0.67240077 -1.44925195 -0.50618838 -1.7259303  -0.0101107\n",
      "   0.58947539 -1.00829823]\n",
      " [-0.83022386  0.32935717 -0.88348662  0.75813921  1.06812995 -0.0294345\n",
      "  -1.3966351  -0.13097041]\n",
      " [ 0.46194396 -1.80321943 -0.00970695  0.241571    0.59000734 -0.45725993\n",
      "   0.17636135 -1.06666958]\n",
      " [-0.29449943 -0.68545972  0.09589121 -0.42960172  0.82950902 -1.58536818\n",
      "  -0.83079874 -0.21816754]\n",
      " [-0.29447958  0.22413771  0.19727941 -0.02606043 -0.07032696  0.95250137\n",
      "   0.54078732 -0.35324619]]\n",
      "biases in layer 2: [0.06934986 0.21186273 0.684796   0.96590163 0.25501542 0.04391858\n",
      " 0.85572842 0.60971052]\n",
      "final output of layer2: [ 0.78773442  3.81759283 -0.92829285  4.49681662 -1.95698579  7.68445591\n",
      "  2.79357561  1.63639858]\n"
     ]
    }
   ],
   "source": [
    "#choosing (weights, neurons) neurons instantiation\n",
    "layer2_neurons = np.random.randn(5,8)\n",
    "#create biases\n",
    "layer2_biases = np.random.rand(8)\n",
    "\n",
    "#calculate output using with layer2_neurons shape in mind (5) * (5,8)\n",
    "layer2_output = np.dot(layer1_output, layer2_neurons) + layer2_biases\n",
    "\n",
    "#printing variables\n",
    "print(\"neurons in layer 2:\\n\",layer2_neurons)\n",
    "print(\"biases in layer 2:\", layer2_biases)\n",
    "print(\"final output of layer2:\", layer2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f351187",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
